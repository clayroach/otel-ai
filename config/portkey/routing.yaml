# Portkey Gateway Routing Configuration
# Based on ADR-014 performance benchmarks
version: "1.0"

# Model targets based on ADR-014 performance data
targets:
  # Primary local model - Best speed/quality/cost balance (10.6s, free)
  - name: "local-primary"
    provider: "openai"  # LM Studio provides OpenAI-compatible API
    models: ["codellama-7b-instruct"]
    endpoint: "http://host.docker.internal:1234/v1"
    
  # Fallback local model - Reliable but slower (19.4s, free)  
  - name: "local-fallback"
    provider: "openai"  # LM Studio provides OpenAI-compatible API
    models: ["sqlcoder-7b-2"]
    endpoint: "http://host.docker.internal:1234/v1"
    
  # Speed-critical API model - Fastest response (6.5s, API cost)
  - name: "api-speed"
    provider: "openai"
    models: ["gpt-3.5-turbo"]
    api_key: "${OPENAI_API_KEY}"
    
  # Quality-critical API model - Best for complex queries (25.5s, high cost)
  - name: "api-quality"
    provider: "anthropic"
    models: ["claude-3-haiku-20240307"]
    api_key: "${ANTHROPIC_API_KEY}"

# Routing rules
routes:
  # Speed-critical queries (require < 7s response)
  - condition: 
      headers:
        x-priority: "high"
        x-response-time-required: "<7000"
    target: "api-speed"
    
  # Complex diagnostic queries requiring sophisticated analysis
  - condition:
      prompt_contains: ["complex", "sophisticated", "detailed analysis", "comprehensive"]
    target: "api-quality"
    
  # SQL-specific queries default to local models
  - condition:
      prompt_contains: ["SELECT", "FROM", "WHERE", "SQL", "query", "ClickHouse"]
    target: "local-primary"
    fallback: ["local-fallback", "api-speed"]
    
  # Default route for all other requests
  - condition: "default"
    target: "local-primary"
    fallback: ["local-fallback", "api-speed"]

# Resilience configuration
retry:
  attempts: 3
  delay: 1000
  backoff: "exponential"
  max_delay: 10000

circuit_breaker:
  threshold: 5
  timeout: 30000
  half_open_requests: 2

# Caching for repeated queries
cache:
  enabled: true
  ttl: 3600  # 1 hour
  semantic_similarity_threshold: 0.95
  max_size: 1000  # Maximum number of cached responses

# Load balancing for local models
load_balancing:
  strategy: "round_robin"  # Options: round_robin, least_latency, random
  health_check_interval: 30000  # 30 seconds

# Observability
observability:
  metrics: true
  tracing: true
  logging: "info"  # Options: debug, info, warn, error