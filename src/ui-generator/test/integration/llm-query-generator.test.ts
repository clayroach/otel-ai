import { describe, it, expect, beforeAll } from "vitest"
import { Effect, Layer, Duration, pipe } from "effect"
import { CriticalPath } from "../../query-generator/types"
// Service imports removed - using direct LLM generation for speed
import { generateQueryWithLLM, ANALYSIS_GOALS, validateGeneratedSQL } from "../../query-generator/llm-query-generator"
import { StorageAPIClientTag } from "../../../storage/api-client"
import { LLMManagerServiceTag } from "../../../llm-manager"
// Model metadata no longer needed - removed model-registry
import { LLMManagerLive } from "../../../llm-manager/llm-manager-live"

// Test data representing a real critical path
const testPath: CriticalPath = {
  id: "checkout-flow",
  name: "E-commerce Checkout Flow",
  services: ["frontend", "cart-service", "payment-service", "inventory-service", "notification-service"],
  startService: "frontend",
  endService: "notification-service",
  metadata: {
    criticality: "high",
    sla: "99.9%"
  }
}

// Check if we should skip tests in CI
const isCI = Boolean(process.env.CI || process.env.GITHUB_ACTIONS)
const shouldSkipTests = isCI

describe.skipIf(shouldSkipTests)("LLM Query Generator", () => {
  let llmAvailable = false
  let llmDetails: {
    endpoint?: string
    model?: string
    status?: string
    error?: string
  } = {}
  let llmConfig: { endpoint: string; model: string } | undefined
  
  beforeAll(async () => {
    // Removed SKIP_LLM_TESTS check - dynamically determine availability based on actual configuration
    
    console.log("üîç Checking LLM availability...")
    
    const endpoint = process.env.LLM_ENDPOINT || "http://localhost:1234/v1"
    
    try {
      // First, fetch available models from the /models endpoint
      console.log(`   Fetching models from: ${endpoint}/models`)
      
      const modelsResponse = await fetch(`${endpoint}/models`)
      if (!modelsResponse.ok) {
        throw new Error(`Failed to fetch models: ${modelsResponse.status} ${modelsResponse.statusText}`)
      }
      
      const modelsData = await modelsResponse.json()
      const availableModels = modelsData.data || []
      
      if (availableModels.length === 0) {
        throw new Error("No models available at endpoint")
      }
      
      console.log(`   Found ${availableModels.length} available models:`)
      availableModels.forEach((model: { id: string; object: string }) => {
        console.log(`     - ${model.id}`)
      })
      
      // Prioritize models for SQL generation based on Portkey config
      let selectedModel = null

      // SQL model selection is now handled by Portkey config
      // Look for SQL-optimized models in available models
      const sqlModelPreferences = [
        'sqlcoder',
        'codellama',
        'deepseek-coder'
      ]
      
      // Try to find a model from the preference list
      for (const preferredModel of sqlModelPreferences) {
        const found = availableModels.find((m: { id: string }) => 
          m.id === preferredModel || 
          m.id.toLowerCase().includes(preferredModel?.toLowerCase() || '')
        )
        if (found) {
          console.log(`     Found preferred SQL model: ${found.id}`)
          selectedModel = found
          break
        }
      }
      
      // Fallback to hardcoded priority if no preferred model found
      if (!selectedModel) {
        selectedModel = availableModels.find((m: { id: string }) => {
          const modelIdLower = m.id.toLowerCase()
          // Metadata no longer available
          
          // First priority: sqlcoder models (actually good at SQL)
          if (modelIdLower.includes('sqlcoder')) {
            console.log(`     Found SQLCoder model: ${m.id}`)
            return true
          }
          // Second priority: codellama models  
          if (m.id === 'codellama-7b-instruct') {
            console.log(`     Found CodeLlama model: ${m.id}`)
            return true
          }
          // Third priority: qwen coder models
          if (modelIdLower.includes('qwen') && modelIdLower.includes('coder')) {
            console.log(`     Found Qwen Coder model: ${m.id}`)
            return true
          }
          // Fourth priority: deepseek-coder models
          if (modelIdLower.includes('deepseek-coder')) {
            console.log(`     Found DeepSeek-Coder model: ${m.id}`)
            return true
          }
          // Skip starcoder for now as it's generating incorrect responses
          return false
        }) || availableModels[0]
      }
      
      console.log(`   Selected model for testing: ${selectedModel.id}`)
      
      // Set environment variables for the test
      const originalEndpoint = process.env.LM_STUDIO_ENDPOINT

      process.env.LM_STUDIO_ENDPOINT = endpoint
      // Model selection now handled by Portkey config, no need to set model env var
      
      // Use the standard LLMManagerLive layer which loads from environment
      
      // Try a real generation request with a minimal prompt for faster health check
      const testRequest = {
        prompt: "OK", // Minimal prompt for fastest possible response
        taskType: "general" as const,
        preferences: {
          model: selectedModel.id,
          maxTokens: 5,   // Minimal tokens for faster response
          temperature: 0
        }
      }
      
      console.log(`   Sending test request to model ${selectedModel.id}...`)
      
      let testResponse
      try {
        testResponse = await Effect.runPromise(
          Effect.gen(function* () {
            const service = yield* LLMManagerServiceTag
            return yield* service.generate(testRequest)
          }).pipe(
            Effect.provide(LLMManagerLive),
            Effect.timeout(Duration.seconds(process.env.NODE_ENV === 'test' || process.env.CI ? 30 : 60)),
            Effect.catchAll((error) => {
              console.log(`   Request failed:`, error)
              return Effect.succeed({ content: '', model: 'error', usage: { promptTokens: 0, completionTokens: 0, totalTokens: 0 } })
            })
          )
        )
        
        console.log(`   Response received:`, JSON.stringify(testResponse, null, 2))
      } finally {
        // Restore original environment variables
        if (originalEndpoint) {
          process.env.LM_STUDIO_ENDPOINT = originalEndpoint
        } else {
          delete process.env.LM_STUDIO_ENDPOINT
        }
        // Model cleanup not needed as we're not modifying model env vars
      }
      
      if (testResponse && testResponse.content) {
        llmAvailable = true
        
        llmDetails = {
          endpoint,
          model: selectedModel.id,
          status: "healthy"
        }
        
        // Set the llmConfig for use in tests
        llmConfig = {
          endpoint,
          model: selectedModel.id
        }
        
        console.log("‚úÖ LLM is available and responding:")
        console.log(`   Endpoint: ${llmDetails.endpoint}`)
        console.log(`   Active Model: ${llmDetails.model}`)
        console.log(`   Available Models: ${availableModels.map((m: { id: string }) => m.id).join(", ")}`)
        console.log(`   Test response: ${testResponse.content.substring(0, 50)}`)
      } else {
        throw new Error("LLM returned empty response")
      }
    } catch (error) {
      llmAvailable = false
      llmDetails = {
        endpoint,
        status: "unavailable",
        error: error instanceof Error ? error.message : String(error)
      }
      console.log("‚ùå LLM is not available:")
      console.log(`   Endpoint: ${endpoint}`)
      console.log(`   Error: ${llmDetails.error}`)
      console.log("   Tests requiring LLM will be skipped")
    }
  }, 90000) // 90 second timeout for beforeAll to allow for LLM health check
  
  describe("LLM Availability", () => {
    it("should report LLM status", () => {
      console.log("\nüìä LLM Status Report:")
      console.log(`   Available: ${llmAvailable}`)
      console.log(`   Details: ${JSON.stringify(llmDetails, null, 2)}`)
      
      // This test always passes - it's just for reporting
      expect(llmDetails).toBeDefined()
    })
  })
  
  describe("Query Generation with Real LLM", () => {
    it("should generate valid SQL for latency analysis", async () => {
      if (!llmAvailable) {
        console.log("   ‚è≠Ô∏è  Skipping: LLM not available")
        return
      }
      const query = await Effect.runPromise(
        pipe(
          generateQueryWithLLM(testPath, ANALYSIS_GOALS.latency, llmConfig),
          Effect.provide(LLMManagerLive)
        )
      )
      
      expect(query.sql).toBeDefined()
      expect(query.description).toBeDefined()
      expect(validateGeneratedSQL(query.sql)).toBe(true)
      
      // Check that it includes the actual service names
      expect(query.sql).toContain("frontend")
      expect(query.sql).toContain("payment-service")
      
      // Should have latency-specific elements appropriate for the analysis goal
      const sqlLower = query.sql.toLowerCase()
      
      // Must have some form of latency/duration aggregation
      const hasLatencyAggregation = 
        sqlLower.includes("quantile") ||
        sqlLower.includes("avg") ||
        sqlLower.includes("percentile") ||
        sqlLower.includes("min") ||
        sqlLower.includes("max")
      expect(hasLatencyAggregation).toBe(true)
      
      // Must reference duration/latency column
      expect(sqlLower).toContain("duration")
      
      // Must filter or group by service
      expect(sqlLower).toContain("service")
      
      // Must have WHERE clause with service filter
      expect(sqlLower).toMatch(/where.*service.*in/)
    })
    
    it("should generate valid SQL for error analysis", async () => {
      if (!llmAvailable) {
        console.log("   ‚è≠Ô∏è  Skipping: LLM not available")
        return
      }
      const query = await Effect.runPromise(
        pipe(
          generateQueryWithLLM(testPath, ANALYSIS_GOALS.errors, llmConfig),
          Effect.provide(LLMManagerLive)
        )
      )
      
      expect(query.sql).toBeDefined()
      expect(validateGeneratedSQL(query.sql)).toBe(true)
      
      // Should have error-specific elements appropriate for error analysis
      const sqlLower = query.sql.toLowerCase()
      const sqlUpper = query.sql.toUpperCase()
      
      // Must reference status/error columns
      expect(sqlLower).toContain("status")
      
      // Must have some error filtering condition
      const hasErrorFilter = 
        sqlUpper.includes("!= 'OK'") ||
        sqlUpper.includes("!= 200") ||
        sqlUpper.includes("ERROR") ||
        sqlUpper.includes("<> 'OK'") ||
        sqlLower.includes("not like '%ok%'")
      expect(hasErrorFilter).toBe(true)
      
      // Should aggregate or count errors
      const hasAggregation = 
        sqlLower.includes("count") ||
        sqlLower.includes("sum") ||
        sqlLower.includes("group by")
      expect(hasAggregation).toBe(true)
    })
    
    it("should generate deterministic queries for same input", { timeout: 30000 }, async () => {
      if (!llmAvailable) {
        console.log("   ‚è≠Ô∏è  Skipping: LLM not available")
        return
      }
      console.log("\nüîÑ Testing determinism with temperature=0...")
      
      // Generate the same query 2 times sequentially to avoid overwhelming the LLM
      const query1 = await Effect.runPromise(pipe(generateQueryWithLLM(testPath, ANALYSIS_GOALS.latency, llmConfig), Effect.provide(LLMManagerLive)))
      const query2 = await Effect.runPromise(pipe(generateQueryWithLLM(testPath, ANALYSIS_GOALS.latency, llmConfig), Effect.provide(LLMManagerLive)))
      
      // Both should be valid
      expect(validateGeneratedSQL(query1.sql)).toBe(true)
      expect(validateGeneratedSQL(query2.sql)).toBe(true)
      
      // For deterministic test, queries should be similar
      // Normalize SQL for comparison (remove extra whitespace, lowercase)
      const normalize = (sql: string) => sql.toLowerCase().replace(/\s+/g, ' ').trim()
      
      const norm1 = normalize(query1.sql)
      const norm2 = normalize(query2.sql)
      
      // With temperature=0, queries should be identical or nearly identical
      // Allow for minor variations in formatting but structure should match
      const similarity = (a: string, b: string) => {
        if (a === b) return 1.0 // Identical
        
        // Check if they have the same clauses in roughly the same order
        const extractClauses = (sql: string) => ({
          select: sql.match(/select\s+([^from]+)/i)?.[1]?.trim(),
          from: sql.match(/from\s+([^where|group|order|limit]+)/i)?.[1]?.trim(),
          where: sql.match(/where\s+([^group|order|limit]+)/i)?.[1]?.trim(),
          groupBy: sql.match(/group\s+by\s+([^order|limit]+)/i)?.[1]?.trim(),
          orderBy: sql.match(/order\s+by\s+([^limit]+)/i)?.[1]?.trim()
        })
        
        const clausesA = extractClauses(a)
        const clausesB = extractClauses(b)
        
        // Count matching clauses
        let matches = 0
        let total = 0
        const keys = ['select', 'from', 'where', 'groupBy', 'orderBy'] as const
        for (const key of keys) {
          if (clausesA[key] || clausesB[key]) {
            total++
            if (clausesA[key] === clausesB[key]) matches++
          }
        }
        
        return total > 0 ? matches / total : 0
      }
      
      const sim12 = similarity(norm1, norm2)
      
      console.log(`   Query similarity: 1-2: ${sim12.toFixed(2)}`)
      
      // With temperature=0, at least 70% similarity expected (relaxed for CodeLlama)
      expect(sim12).toBeGreaterThanOrEqual(0.7)
      
      // Check structural similarity
      const countClauses = (sql: string) => ({
        select: (sql.match(/SELECT/gi) || []).length,
        from: (sql.match(/FROM/gi) || []).length,
        where: (sql.match(/WHERE/gi) || []).length,
        groupBy: (sql.match(/GROUP BY/gi) || []).length
      })
      
      const structure1 = countClauses(query1.sql)
      const structure2 = countClauses(query2.sql)
      
      console.log("   Query 1 structure:", structure1)
      console.log("   Query 2 structure:", structure2)
      
      // Structures should be similar (allow some variation for CodeLlama)
      expect(structure1.select).toEqual(structure2.select)
      expect(structure1.from).toEqual(structure2.from)
    })
    
    it("should generate different queries for different analysis goals", { timeout: 30000 }, async () => {
      if (!llmAvailable) {
        console.log("   ‚è≠Ô∏è  Skipping: LLM not available")
        return
      }
      
      // Run queries sequentially to avoid overwhelming local LLM server
      // While parallel execution is better in theory, local LLM servers often struggle with concurrent requests
      const latencyQuery = await Effect.runPromise(
        pipe(
          generateQueryWithLLM(testPath, ANALYSIS_GOALS.latency, llmConfig),
          Effect.provide(LLMManagerLive)
        )
      )
      
      const errorQuery = await Effect.runPromise(
        pipe(
          generateQueryWithLLM(testPath, ANALYSIS_GOALS.errors, llmConfig),
          Effect.provide(LLMManagerLive)
        )
      )
      
      const bottleneckQuery = await Effect.runPromise(
        pipe(
          generateQueryWithLLM(testPath, ANALYSIS_GOALS.bottlenecks, llmConfig),
          Effect.provide(LLMManagerLive)
        )
      )
      
      // All should be valid
      expect(validateGeneratedSQL(latencyQuery.sql)).toBe(true)
      expect(validateGeneratedSQL(errorQuery.sql)).toBe(true)
      expect(validateGeneratedSQL(bottleneckQuery.sql)).toBe(true)
      
      // Each should have different characteristics
      // Latency query should have duration/time analysis
      const hasLatencyFocus = 
        latencyQuery.sql.toLowerCase().includes("quantile") ||
        latencyQuery.sql.toLowerCase().includes("avg") ||
        latencyQuery.sql.toLowerCase().includes("duration")
      expect(hasLatencyFocus).toBe(true)
      
      // Error query should filter for errors
      const hasErrorFocus = 
        errorQuery.sql.toUpperCase().includes("!= 'OK'") ||
        errorQuery.sql.toUpperCase().includes("!= 200") ||
        errorQuery.sql.toLowerCase().includes("error") ||
        errorQuery.sql.toLowerCase().includes("status")
      expect(hasErrorFocus).toBe(true)
      
      // Bottleneck query should look at operations or performance
      const hasBottleneckFocus = 
        bottleneckQuery.sql.toLowerCase().includes("operation") ||
        bottleneckQuery.sql.toLowerCase().includes("max") ||
        bottleneckQuery.sql.toLowerCase().includes("slow")
      expect(hasBottleneckFocus).toBe(true)
      
      // SQLs should be different
      expect(latencyQuery.sql).not.toEqual(errorQuery.sql)
      expect(errorQuery.sql).not.toEqual(bottleneckQuery.sql)
    })
    
    it("should handle custom analysis goals", { timeout: 30000 }, async () => {
      if (!llmAvailable) {
        console.log("   ‚è≠Ô∏è  Skipping: LLM not available")
        return
      }
      const customGoal = "Analyze the relationship between service latency and error rates, focusing on correlation patterns during peak load"
      
      const query = await Effect.runPromise(
        pipe(
          generateQueryWithLLM(testPath, customGoal, llmConfig),
          Effect.provide(LLMManagerLive)
        )
      )
      
      expect(query.sql).toBeDefined()
      expect(validateGeneratedSQL(query.sql)).toBe(true)
      expect(query.description).toBeDefined()
      
      console.log("\nüìù Custom goal query generated:")
      console.log(`   Goal: ${customGoal.substring(0, 50)}...`)
      console.log(`   Description: ${query.description}`)
    })
    
    it.skip("should properly escape service names to prevent SQL injection", { timeout: 30000 }, async () => {
      // KNOWN ISSUE: SQL-specific models (sqlcoder, codellama) do not properly escape
      // SQL injection attempts in service names. This is a limitation of these models
      // that are trained on SQL patterns but not security best practices.
      // TODO: Implement pre-processing sanitization layer before sending to SQL models

      if (!llmAvailable) {
        console.log("   ‚è≠Ô∏è  Skipping: LLM not available")
        return
      }
      const maliciousPath: CriticalPath = {
        ...testPath,
        services: ["frontend' OR '1'='1", "backend"]
      }

      const query = await Effect.runPromise(
        pipe(
          generateQueryWithLLM(maliciousPath, ANALYSIS_GOALS.latency, llmConfig),
          Effect.provide(LLMManagerLive)
        )
      )

      // The service name should be escaped or quoted properly
      expect(query.sql).not.toContain("OR '1'='1") // Should not appear unescaped
      expect(validateGeneratedSQL(query.sql)).toBe(true)
    })
  })
  
  describe("Integration with Storage Layer", () => {
    // Mock storage that returns predictable data
    const mockStorageAPIClient = Layer.succeed(
      StorageAPIClientTag,
      {
        writeOTLP: () => Effect.succeed(undefined),
        queryTraces: () => Effect.succeed([]),
        queryMetrics: () => Effect.succeed([]),
        queryLogs: () => Effect.succeed([]),
        queryAI: () => Effect.succeed([]),
        queryRaw: (sql: string) => {
          // Return different mock data based on query patterns
          if (sql.includes("status_code != 'OK'")) {
            return Effect.succeed([
              { service_name: "payment-service", status_code: "ERROR", error_count: 5 }
            ])
          } else if (sql.includes("quantile")) {
            return Effect.succeed([
              { service_name: "payment-service", p50_ms: 250, p95_ms: 450, p99_ms: 490 },
              { service_name: "frontend", p50_ms: 140, p95_ms: 180, p99_ms: 195 }
            ])
          } else {
            return Effect.succeed([
              { service_name: "frontend", request_count: 1000 }
            ])
          }
        },
        healthCheck: () => Effect.succeed({ clickhouse: true, s3: true })
      }
    )
    
    it("should execute generated queries and return results", { timeout: 30000 }, async () => {
      if (!llmAvailable) {
        console.log("   ‚è≠Ô∏è  Skipping: LLM not available")
        return
      }
      // Test with a single query instead of generating all queries (much faster)
      const singleQuery = await Effect.runPromise(
        pipe(
          generateQueryWithLLM(testPath, ANALYSIS_GOALS.latency, llmConfig),
          Effect.provide(LLMManagerLive)
        )
      )

      expect(singleQuery).toBeDefined()
      expect(singleQuery.sql).toBeDefined()

      // Test execution through storage layer
      const program = Effect.gen(function* () {
        const storage = yield* StorageAPIClientTag
        const result = yield* storage.queryRaw(singleQuery.sql)
        return result
      })

      const result = await Effect.runPromise(
        Effect.provide(program, mockStorageAPIClient)
      )

      expect(result).toBeDefined()
      expect(Array.isArray(result)).toBe(true)
    })
    
    it("should handle query execution errors gracefully", { timeout: 30000 }, async () => {
      if (!llmAvailable) {
        console.log("   ‚è≠Ô∏è  Skipping: LLM not available")
        return
      }

      // Generate a single query first (faster)
      const singleQuery = await Effect.runPromise(
        pipe(
          generateQueryWithLLM(testPath, ANALYSIS_GOALS.latency, llmConfig),
          Effect.provide(LLMManagerLive)
        )
      )

      // Create failing storage that will error on query execution
      const failingStorage = Layer.succeed(
        StorageAPIClientTag,
        {
          writeOTLP: () => Effect.succeed(undefined),
          queryTraces: () => Effect.succeed([]),
          queryMetrics: () => Effect.succeed([]),
          queryLogs: () => Effect.succeed([]),
          queryAI: () => Effect.succeed([]),
          queryRaw: () => Effect.fail({
            _tag: 'QueryError' as const,
            message: "ClickHouse connection failed",
            query: singleQuery.sql,
            error: new Error("Network timeout")
          }),
          healthCheck: () => Effect.succeed({ clickhouse: false, s3: false })
        }
      )

      // Create a simple query executor
      const program = Effect.gen(function* () {
        const storage = yield* StorageAPIClientTag

        // Try to execute - should fail gracefully
        const result = yield* Effect.either(storage.queryRaw(singleQuery.sql))

        if (result._tag === 'Left') {
          // Error case - expected
          return { error: result.left.message, data: [] }
        } else {
          // Success case - unexpected in this test
          return { error: undefined, data: result.right }
        }
      })

      const result = await Effect.runPromise(
        Effect.provide(program, failingStorage)
      )

      expect(result.error).toBeDefined()
      expect(result.error).toContain("ClickHouse connection failed")
      expect(result.data).toEqual([])
    })
  })
  
  describe("SQL Validation", () => {
    it("should validate that generated SQL contains required elements", () => {
      const validSQL = `
        SELECT service_name, count(*) as cnt 
        FROM traces 
        WHERE service_name IN ('service1', 'service2')
        GROUP BY service_name
      `
      expect(validateGeneratedSQL(validSQL)).toBe(true)
    })
    
    it("should reject SQL with dangerous operations", () => {
      const dangerousSQL = `
        DROP TABLE traces;
        SELECT * FROM traces WHERE service_name = 'test'
      `
      expect(validateGeneratedSQL(dangerousSQL)).toBe(false)
    })
    
    it("should reject SQL missing required elements", () => {
      const incompleteSQL = `
        service_name, count(*) as cnt 
        WHERE service_name IN ('service1')
      `
      expect(validateGeneratedSQL(incompleteSQL)).toBe(false)
    })
  })
})